{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The pricing model utilises LSV model with calibration, to price autocallables with three underlyings. Since these models often take a long time to train under Monte Carlo simulation, we attempt to price these derivatives with machine learning models in a more swiftly manner.\n",
    "\n",
    "In this notebook, we would be doing four things:\n",
    "1. Data preprocessing\n",
    "2. Building the neural network\n",
    "3. Explanability\n",
    "4. Making the predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Data engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We first do EDA and find out that 14541000 rows of data are given to us through a generator. Since a large chunk of data is given to us, we cannot use StandardScaler in scikit-learn due to limited memory. Hence, we do the following things:\n",
    "\n",
    "1. We loop through all 14541000 rows to get mean and standard deviation of each column and paste the values in the cell below.\n",
    "2. We standard-scale all of the columns (target included).\n",
    "3. We shuffle each batch of data given to us to ensure a smooth learning curve.\n",
    "\n",
    "NB The parameter, 'train' in engineerData function is set as True by default when we train our model. It will return the preprocessed inputs and targets. When the function is used to make predictions, 'train' should be set as False to return only the preprocessed inputs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Getting the mean and standard deviation of each column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = []\n",
    "std_values = []\n",
    "\n",
    "# Getting column names of dataset\n",
    "dataGen = dl.batch(fromRow=1, toRow=14541000)\n",
    "df = next(dataGen)\n",
    "columns = df.columns\n",
    "\n",
    "# Looping through each column \n",
    "for col in columns:\n",
    "    df = pd.Series()\n",
    "    dataGen = dl.batch(fromRow=1, toRow=14541000)\n",
    "    \n",
    "    # Looping through each batch of data\n",
    "    for _, d in enumerate(dataGen):\n",
    "        df.append(d[col])\n",
    "    \n",
    "    # Appending the mean and standard deviation values to list\n",
    "    mean_values.append(df.mean())\n",
    "    std_values.append(df.std())"
   ]
  },
  {
   "source": [
    "## Function for data preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineerData(df, train=True):\n",
    "    \n",
    "    import random\n",
    "    random.seed(10)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(10)\n",
    "    \n",
    "    # Standard Scaling \n",
    "    # mean_values, std_values obtained from above \n",
    "    df = df.astype(float) \n",
    "    for i, j in enumerate(df.columns):\n",
    "        df[j] = (df[j] - mean_values[i]) / std_values[i]\n",
    "\n",
    "    # Return inputs and targets if train, else return inputs only\n",
    "    if train:\n",
    "        # Shuffle within the dataset\n",
    "        from sklearn.utils import shuffle\n",
    "        df = shuffle(df)\n",
    "        return df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We use an artificial neural network to model the price. Liu, Oosterlee and Bohte (2018) shows that an aritificial neural network can approximate the Heston stochastic volatility model well with a MSE of 1.65e-8 on test set.\n",
    "\n",
    "We use an artificial neural network of 10 hidden layers and 100 neurons in each layer since it gives us a smaller bias than models with less layers and more layers will prevent the model from converging and takes too much to time on a 4GB machine.\n",
    "\n",
    "We use RELU activation, which is a standard activation nowadays and we use He initialization which is a better initialization than Xavier or random initialization when matched with RELU by He, Zhang, Ren and Sun (2015).\n",
    "\n",
    "We use Adam optimizer which is also a standard optimizer nowadays, and optimize on MSE as it is not practical to optimize on Maximum Absolute Error.\n",
    "\n",
    "\n",
    "S. Liu, C. Oosterlee, M. Bohte.“Pricing options and computing implied volatilities using neural networks” https://arxiv.org/abs/1901.08943 (2018)\n",
    "\n",
    "K. He, X. Zhang, S. Ren, J. Sun.“Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification” https://arxiv.org/abs/1502.01852 (2015)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import random\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# Building the neural network\n",
    "model = Sequential()\n",
    "\n",
    "for i in range(10):\n",
    "    random.seed(10)\n",
    "    set_random_seed(10)\n",
    "    model.add(Dense(units=100, activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "model.add(Dense(units=1, activation='linear'))"
   ]
  },
  {
   "source": [
    "When the model loss meets the plateau, we use a smaller learning rate to train. The following picture is created by He (2015) and arrows added by Hammel (2019)\n",
    "\n",
    "K. He, X. Zhang, S. Ren, J. Sun. \"Deep Residual Learning for Image Recognition\" \n",
    "https://arxiv.org/abs/1512.03385 (2015)\n",
    "\n",
    "B. D. Hammel. 2019. What Learning Rate Should I Use?. online \n",
    "Available at: <http://www.bdhammel.com/assets/learning-rate/resnet_loss.png>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<img src=\"http://www.bdhammel.com/assets/learning-rate/resnet_loss.png\"/>",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# Manually change to 0.0001 and 0.00001 when training loss plateaus\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001)) "
   ]
  },
  {
   "source": [
    "Since there are significant correlations within each batch, we carry out stratified sampling when we train our model. For every epoch, the data drawn from the training set is in different order to ensure more uniform sampling.\n",
    "\n",
    "1. We first get 1% of data from each batch as validation dataset.\n",
    "2. We pull out 2.5% of data from each batch without replacement in every epoch and train our neural network with it. (40 times in each epoch)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length in each batch\n",
    "batchlength = []\n",
    "for d in dataGen:\n",
    "    batchlength.append(len(d))\n",
    "\n",
    "# Get the indexes of validation dataset\n",
    "listofindex = []\n",
    "for d in batchlength:\n",
    "    d = [i for i in range(d)]\n",
    "    random.shuffle(d)\n",
    "    listofindex.append(d[:int(len(d)*0.01)])\n",
    "    \n",
    "# Training for 100 epochs\n",
    "epochs = 100\n",
    "splits = 40\n",
    "for n in range(epoch):\n",
    "    # Split each batch into 40 portions and get the indexes\n",
    "    for i, d in enumerate(batchlength):\n",
    "        d = [a for a in range(d) if a not in listofindex[i]]\n",
    "        random.shuffle(d)\n",
    "        batch.append(np.array_split(d, splits))\n",
    "    \n",
    "    # Loop for 40 times to train through the whole train dataset as we have limited memory\n",
    "    for j in range(splits):\n",
    "        train = pd.DataFrame()\n",
    "        valid = pd.DataFrame()\n",
    "        dataGen = dl.batch(fromRow=1, toRow=14541000)\n",
    "        for i, d in enumerate(dataGen):\n",
    "            d = d.reset_index(drop=True)\n",
    "            valid = pd.concat([valid, d.iloc[listofindex[i]]])\n",
    "            train = pd.concat([train, d.iloc[batch[i][j]]])\n",
    "\n",
    "        # Transform the data to numpy arrays\n",
    "        trainX, trainY = engineerData(train)\n",
    "        trainX = trainX.values\n",
    "        trainY = trainY.values\n",
    "\n",
    "        del train\n",
    "\n",
    "        validX, validY = engineerData(valid)\n",
    "        validX = validX.values\n",
    "        validY = validY.values\n",
    "\n",
    "        model.fit(trainX, trainY, batch_size=64, epochs=1, validation_data=(validX, validY))\n",
    "\n",
    "        # Free up enough memory for training\n",
    "        del trainX, trainY, validX, validY"
   ]
  },
  {
   "source": [
    "# 3. Explanability\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since there is no built-in attribute of feature importance for a neural network, we will use another approach to solve the problem.\n",
    "\n",
    "Fisher, Rudin, and Dominici (2018) proposed a method to understand feature importance of different models. \n",
    "\n",
    "1. We get the Mean Squared Error of the original model.\n",
    "2. We shuffle the values within one column and train the model with other columns remain unaltered.\n",
    "3. We get the Squared Mean Error of the new model and divide it by the original error.\n",
    "4. We repeat step 2 and 3 until all columns have been shuffled.\n",
    "5. We rank the error of each model and the model with the higher error indicates that the importance of the feature would be higher.\n",
    "\n",
    "Due to limited time for training, we would train each column with 5% of the dataset and 2 epochs for illustrative purpose. More data should be trained to show the 'real' feature importance. \n",
    "\n",
    "It will take hours to load the following snippet.\n",
    "\n",
    "A. Fisher, C. Rudin, F. Dominici.“All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously” http://arxiv.org/abs/1801.01489 (2018)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(10)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(10)\n",
    "import alphien\n",
    "dl = alphien.data.DataLoader()\n",
    "\n",
    "# Get names of the columns\n",
    "dataGen = dl.batch(fromRow=1, toRow=10)\n",
    "columns = next(dataGen).columns\n",
    "\n",
    "# Get 5% of the dataset to train\n",
    "df = pd.DataFrame()\n",
    "dataGen = dl.batch(fromRow=1, toRow=14541000)\n",
    "for i, d in enumerate(dataGen):\n",
    "    d = d.sample(frac=0.05)\n",
    "    df = pd.concat([df,d])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "explanability = []\n",
    "\n",
    "# Shuffle each column and train the dataset with shuffled columns\n",
    "for col in columns:\n",
    "    # Getting the column data under investigation\n",
    "    dataGen = dl.batch(fromRow=1, toRow=14541000)\n",
    "    augmented = pd.Series()\n",
    "    while True:\n",
    "        try:\n",
    "            data = next(dataGen)\n",
    "            augmented = pd.concat([augmented, data[col]])\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    augmented = augmented.sample(frac=1)\n",
    "    \n",
    "    # Neural network model\n",
    "    model = Sequential()\n",
    "    for i in range(10):\n",
    "        model.add(Dense(units=100, activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    # Swap the original column with a shuffled column\n",
    "    df[col] = augmented.sample(n=df.shape[0]).reset_index(drop=True)\n",
    "\n",
    "    trainX, trainY = engineerData(df)\n",
    "\n",
    "    trainX = trainX.values\n",
    "    trainY = trainY.values\n",
    "    \n",
    "    history = model.fit(trainX, trainY, batch_size=64, epochs=1)\n",
    "    explanability.append(list(history.history.values())[0][0])\n",
    "    \n",
    "    del trainX, trainY\n",
    "    \n",
    "# Show top 10 most important features\n",
    "explanability = [i - min(explanability) for i in explanability]\n",
    "values = sorted(explanability)[-11:-1]\n",
    "highest = sorted(range(len(explanability)), key=lambda i: columns[i])[-11:-1]\n",
    "from operator import itemgetter\n",
    "highest = list(itemgetter(*highest)(columns))\n",
    "\n",
    "# Plot the top 10 important features\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Top 10 important features')\n",
    "plt.barh(highest, values)"
   ]
  },
  {
   "source": [
    "It is shown that the vov (volatility of variance) of underlying2 will lead to the highest influence to the price, followed by the monthly volatility of underlying2 from 6Y, 5Y, 4Y, etc. with the exception of 6M.\n",
    "\n",
    "We noticed that for some features, the loss is slightly lower than the original model. \n",
    "This shows that it is possible that some features are not useful in predicting the price."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### We also look for a linear relationship between the features and the price through fitting it to a multiple linear regression model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import alphien\n",
    "dl = alphien.data.DataLoader()\n",
    "\n",
    "# Randomly generate 5% data \n",
    "df = pd.DataFrame()\n",
    "dataGen = dl.batch(fromRow=1, toRow=14541000)\n",
    "for i, d in enumerate(dataGen):\n",
    "    d = d.sample(frac=0.03)\n",
    "    df = pd.concat([df,d])\n",
    "\n",
    "trainX, trainY = engineerData(df)\n",
    "trainX = trainX.values\n",
    "trainY = trainY.values\n",
    "del df\n",
    "    \n",
    "    \n",
    "# Fitting multiple linear regression model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "trainX = sm.add_constant(trainX)\n",
    "model = sm.OLS(trainY, trainX).fit()\n",
    "model.summary()\n",
    "\n",
    "# Plotting Top 10 features with highest p values\n",
    "columns = d.columns\n",
    "values = sorted(model.pvalues)[-11:-1]\n",
    "highest = sorted(range(len(model.pvalues)), key=lambda i: columns[i])[-11:-1]\n",
    "from operator import itemgetter\n",
    "highest = list(itemgetter(*highest)(columns))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Top 10 features with highest p values')\n",
    "plt.barh(highest, values)"
   ]
  },
  {
   "source": [
    "From the summary, we see that the columns are the same with top 10 important features. Since we only trained the neural network for explanability for 10% of data for 1 epoch due to limited time, the similarity in columns may arise due to the fact that the neural network recognises obvious linear relationship in its early stages of training.\n",
    "\n",
    "None of the columns is statistically significant (p-value > 0.9) to have a linear effect with the price, so the linear relationship between columns and price is inconclusive."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Making predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPredictionFunc(newData, engineerData, model):\n",
    "\n",
    "    transformed_data = engineerData(newData, train=False) #Apply data transform    \n",
    "    return model.predict(transformed_data.values) * 0.011214939016011491 + 0.008983811613634216 #Inverse transform scaled target"
   ]
  },
  {
   "source": [
    "## Appendix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From the names of the columns, we deduce the followings.\n",
    "\n",
    "\n",
    "Input variables:\n",
    "\n",
    "'Undl0volatm_6M' (6-month monthly volatility for underlying asset 0)\n",
    "\n",
    "'Undl0skew95.105_6M' (6-month for differences in implied volatilities at strikes of 95% and 105% for underlying asset 0)\n",
    "\n",
    "'Undl0curve95.105_6M' (6-month for differences in curve at strikes of 95% and 105% for underlying asset 0)\n",
    "\n",
    "'Undl0vov' (Volatility of variance process for underlying asset 0)\n",
    "\n",
    "'Undl0svc' (Correlation of Wiener processes for asset price and variance for underlying asset 0)\n",
    "\n",
    "'Undl0mr' (Speed of mean reversion for underlying asset 0)\n",
    "\n",
    "'ContractFeature_Autocall,BarrierLevel,LevelInitial' (Initial Early redemption level)\n",
    "\n",
    "'ContractFeature_ExpiryPayment,BarrierLevel' (Barrier for kick-in event)\n",
    "\n",
    "'ContractFeature_ExpiryPayment,KickInPaymentGearing_ENCODED' (Leverage level upon kick-in event)\n",
    "\n",
    "'ContractFeature_Schedule,EndDate_ENCODED' (Maturity)\n",
    "\n",
    "'ContractFeature_Schedule,PeriodFrequency_ENCODED' (Frequency of coupon payments per year)\n",
    "\n",
    "\n",
    "Target: \n",
    "\n",
    "'val_lvsvcharge' (Price)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}